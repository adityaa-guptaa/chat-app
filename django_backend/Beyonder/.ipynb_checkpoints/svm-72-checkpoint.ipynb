{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "927acdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b8c5187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dfe8c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "train.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0740c6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "additional_stop_words = {'u', 'im', 'not', 'no', 'never', 'neither', 'nor'}\n",
    "stop_words |= additional_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8eccbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Dictionary for common typos and slangs\n",
    "typos_slangs = {\n",
    "    \"dont\": \"don't\",\n",
    "    \"cant\": \"can't\",\n",
    "    \"lol\": \"laugh out loud\",\n",
    "    \"brb\": \"be right back\",\n",
    "    \"jk\": \"just kidding\",\n",
    "    # Add more typos and slangs as needed\n",
    "}\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    \n",
    "    \n",
    "    # Replace typos and slangs\n",
    "    for typo, correction in typos_slangs.items():\n",
    "        text = text.replace(typo, correction)\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Lemmatization and Stemming\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, pos=pos[0].lower()) if pos[0].lower() in ['n', 'v', 'a'] else lemmatizer.lemmatize(token) for token, pos in tagged_tokens]\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in lemmatized_tokens]\n",
    "    \n",
    "    processed_tokens = []\n",
    "    negation = False\n",
    "    for token in stemmed_tokens:\n",
    "        if token in {'not', 'no', 'never', 'neither', 'nor', \"cannot\", \"won't\"}:\n",
    "            negation = True\n",
    "        elif negation:\n",
    "            token = 'not_' + token\n",
    "            negation = False\n",
    "        processed_tokens.append(token)\n",
    "    \n",
    "    processed_text = ' '.join([word for word in processed_tokens if word not in stop_words])\n",
    "    \n",
    "    return processed_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afb7fbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(clean_text)\n",
    "test['text'] = test['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5e52f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train['text'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75181096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<27480x145017 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 355533 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0e1ba5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['sentiment']\n",
    "y_test = test['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ad5bd5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model training\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb840767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (SVM with TF-IDF):  72.77872099603849\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation\n",
    "test_pred = svm_model.predict(X_test_tfidf)\n",
    "accuracy = accuracy_score(y_test, test_pred)\n",
    "print(\"Accuracy (SVM with TF-IDF): \", accuracy * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7928968d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: horror, Processed Input: horror, Predicted class label: ['negative']\n",
      "Input: better, Processed Input: better, Predicted class label: ['positive']\n"
     ]
    }
   ],
   "source": [
    "# Example predictions\n",
    "input_sentences = [\"horror\", \"better\"] \n",
    "for input_sentence in input_sentences:\n",
    "    processed_input = clean_text(input_sentence)\n",
    "    X_input = tfidf_vectorizer.transform([processed_input])\n",
    "    predicted_label = svm_model.predict(X_input)\n",
    "    print(f\"Input: {input_sentence}, Processed Input: {processed_input}, Predicted class label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4643ce18",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[100:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a101143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open('svm_classifier.pkl', 'wb') as f:\n",
    "    pickle.dump((tfidf_vectorizer, svm_model), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8842d9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "# Load\n",
    "with open('svm_classifier.pkl', 'rb') as f:\n",
    "    tfidf_loaded, svm_model_loaded = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8469850e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: best, Processed Input: best, Predicted class label: ['positive']\n",
      "Input: i am bad what the hell, Processed Input: bad hell, Predicted class label: ['negative']\n"
     ]
    }
   ],
   "source": [
    "# Example predictions\n",
    "input_sentences = [\"best\", \"i am bad what the hell\"] \n",
    "for input_sentence in input_sentences:\n",
    "    processed_input = clean_text(input_sentence)\n",
    "    X_input = tfidf_loaded.transform([processed_input])\n",
    "    predicted_label = svm_model_loaded.predict(X_input)\n",
    "    print(f\"Input: {input_sentence}, Processed Input: {processed_input}, Predicted class label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "239ecea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<27480x146828 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 350629 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bff084",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
