{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3a6f579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd5fd8ed-d18d-43e3-b8e8-677274178d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nb_classifier import NaiveBayesClassifier  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00f6079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pd.read_csv('processed_train_data.csv')\n",
    "test = pd.read_csv('processed_test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c39c9df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "train.dropna(inplace=True)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "additional_stop_words = {'u', 'im'}\n",
    "stop_words |= additional_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2806a6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Dictionary for common typos and slangs\n",
    "typos_slangs = {\n",
    "    \"dont\": \"don't\",\n",
    "    \"cant\": \"can't\",\n",
    "    \"lol\": \"laugh out loud\",\n",
    "    \"brb\": \"be right back\",\n",
    "    \"jk\": \"just kidding\",\n",
    "    # Add more typos and slangs as needed\n",
    "}\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Use raw strings (prefix with r) to prevent SyntaxWarnings\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub(r'\\n', '', text)\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "    \n",
    "    # Replace typos and slangs\n",
    "    for typo, correction in typos_slangs.items():\n",
    "        text = text.replace(typo, correction)\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    lemmatized_tokens = [\n",
    "        lemmatizer.lemmatize(token, pos=pos[0].lower()) if pos[0].lower() in ['n', 'v', 'a']\n",
    "        else lemmatizer.lemmatize(token)\n",
    "        for token, pos in tagged_tokens\n",
    "    ]\n",
    "\n",
    "    # Handle negations\n",
    "    processed_tokens = []\n",
    "    negation = False\n",
    "    for token in lemmatized_tokens:\n",
    "        if token in {'not', 'no', 'never', 'neither', 'nor', \"cannot\", \"won't\"}:\n",
    "            negation = True\n",
    "        elif negation:\n",
    "            token = 'not_' + token\n",
    "            negation = False\n",
    "        processed_tokens.append(token)\n",
    "    \n",
    "    processed_text = ' '.join([word for word in processed_tokens if word not in stop_words])\n",
    "    \n",
    "    return processed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23d3ca00",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(clean_text)\n",
    "test['text'] = test['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6013a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Naive Bayes Classifier from scratch\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self):\n",
    "        self.vocab = set()\n",
    "        self.class_word_counts = {}\n",
    "        self.class_counts = {}\n",
    "    \n",
    "    def train(self, X_train, y_train):\n",
    "        for x, y in zip(X_train, y_train):\n",
    "            if y not in self.class_word_counts:\n",
    "                self.class_word_counts[y] = {}\n",
    "                self.class_counts[y] = 0\n",
    "            self.class_counts[y] += 1\n",
    "            words = x.split()\n",
    "            for word in words:\n",
    "                self.vocab.add(word)\n",
    "                if word not in self.class_word_counts[y]:\n",
    "                    self.class_word_counts[y][word] = 0\n",
    "                self.class_word_counts[y][word] += 1\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        y_pred = []\n",
    "        for x in X_test:\n",
    "            max_prob = float('-inf')\n",
    "            best_class = None\n",
    "            for cls in self.class_counts.keys():\n",
    "                prob = self.calculate_class_probability(x, cls)\n",
    "                if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "                    best_class = cls\n",
    "            y_pred.append(best_class)\n",
    "        return y_pred\n",
    "    \n",
    "    def calculate_class_probability(self, x, cls):\n",
    "        log_prob = np.log(self.class_counts[cls]) - np.log(sum(self.class_counts.values()))\n",
    "        words = x.split()\n",
    "        for word in words:\n",
    "            if word in self.vocab:\n",
    "                log_prob += np.log(self.calculate_word_probability(word, cls))\n",
    "        return log_prob\n",
    "    \n",
    "    def calculate_word_probability(self, word, cls):\n",
    "        count_word_cls = self.class_word_counts[cls].get(word, 0) + 1  # Laplace smoothing\n",
    "        count_all_cls = sum(self.class_word_counts[cls].values()) + len(self.vocab)\n",
    "        return count_word_cls / count_all_cls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64ce7288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction\n",
    "X_train = train['text']\n",
    "X_test = test['text']\n",
    "y_train = train['sentiment']\n",
    "y_test = test['sentiment']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0526acf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                            id respond go\n",
       "1                                  sooo sad miss san diego\n",
       "2                                                bos bully\n",
       "3                                    interview leave alone\n",
       "4                      son couldnt put release already buy\n",
       "                               ...                        \n",
       "27475    wish could come see u denver husband lose job ...\n",
       "27476    ive wonder rake client make clear net n't forc...\n",
       "27477    yay good enjoy break probably need hectic week...\n",
       "27478                                                worth\n",
       "27479                           flirt go atg smile yay hug\n",
       "Name: text, Length: 27413, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6837c793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training Naive Bayes Classifier\n",
    "# nb_classifier = NaiveBayesClassifier()\n",
    "nb_classifier.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d1c1cf8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Prediction\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m y_pred = \u001b[43mnb_classifier\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mNaiveBayesClassifier.predict\u001b[39m\u001b[34m(self, X_test)\u001b[39m\n\u001b[32m     25\u001b[39m best_class = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.class_counts.keys():\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     prob = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcalculate_class_probability\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m prob > max_prob:\n\u001b[32m     29\u001b[39m         max_prob = prob\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mNaiveBayesClassifier.calculate_class_probability\u001b[39m\u001b[34m(self, x, cls)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcalculate_class_probability\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;28mcls\u001b[39m):\n\u001b[32m     35\u001b[39m     log_prob = np.log(\u001b[38;5;28mself\u001b[39m.class_counts[\u001b[38;5;28mcls\u001b[39m]) - np.log(\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m.class_counts.values()))\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     words = \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m()\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words:\n\u001b[32m     38\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.vocab:\n",
      "\u001b[31mAttributeError\u001b[39m: 'float' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "y_pred = nb_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a499715c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(\"Accuracy (Naive Bayes from scratch): \", accuracy * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15dcd2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: highly appreciated, Processed Input: highly appreciate, Predicted class label: positive\n",
      "Input: hell, Processed Input: hell, Predicted class label: negative\n"
     ]
    }
   ],
   "source": [
    "# Example predictions\n",
    "input_sentences = [\"highly appreciated\", \"hell\"] \n",
    "for input_sentence in input_sentences:\n",
    "    processed_input = clean_text(input_sentence)\n",
    "    predicted_label = nb_classifier.predict([processed_input])[0]\n",
    "    print(f\"Input: {input_sentence}, Processed Input: {processed_input}, Predicted class label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6691df0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open('naive_b_classifier.pkl', 'wb') as f:\n",
    "    pickle.dump(nb_classifier, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfbfc88-d3ac-4dd6-b49e-509d00a8214b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
